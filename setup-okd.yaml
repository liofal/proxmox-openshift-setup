---
- name: Setting up Linux server
  hosts: service
  vars_files:
    - vars/main.yaml
    - vars/nodes.yaml
    - vars/network.yaml
    - vars/okd-version.yaml
  handlers:
    - import_tasks: handlers/main.yaml
  become: yes
  become_method: sudo
  gather_facts: no
  tasks:
    - name: Checking every 5 seconds if the server is ready (timeout 300s)
      wait_for_connection:
        delay: 5
        timeout: 300

    - import_tasks: tasks/os-config.yaml
      tags: ["os-config"]
    - import_tasks: tasks/downloads.yaml
      tags: ["download"]

    # tasks/structure.yaml
    # - name: Create directories
    #   file:
    #     path: "{{ item.name }}"
    #     state: directory
    #     owner: "{{ item.owner }}"
    #     mode: 0755 # For some reason, item.mode converts 0755 into 493 ... weird
    #   loop:
    #     - { name: /var/www/html/okd4, owner: root, mode: 0755 }
    #     - { name: /etc/named/zones, owner: root, mode: 0755 }
    #     - { name: tmp, owner: ansible, mode: 0755 }
    #     - { name: install_dir, owner: ansible, mode: 0755 }
    #     - { name: /var/lib/tftpboot/fcos, owner: root, mode: 0755 }
    #     - { name: /var/lib/tftpboot/pxelinux.cfg, owner: root, mode: 0755 }
    #     - { name: /var/nfsshare/registry, owner: nobody, mode: 0777 }
    #     - { name: /etc/dhcp, owner: root, mode: 0755 }

    # # tasks/dns.yaml
    # - name: Fix NetworkManager.cnf
    #   lineinfile:
    #     path: /etc/NetworkManager/NetworkManager.conf
    #     insertafter: "[main]"
    #     line: "dns=none"
    #   notify:
    #     - Restart DNS

    # # Build DNS configuration
    # - name: Create config files
    #   template:
    #     src: "{{ item.src }}"
    #     dest: "{{ item.dest }}"
    #     owner: root
    #     mode: 0644
    #   with_items:
    #     - { src: templates/dns-forward.j2, dest: /etc/named/zones/db.forward }
    #     - { src: templates/dns-reverse.j2, dest: /etc/named/zones/db.reverse }
    #     - { src: templates/named.conf.j2, dest: /etc/named.conf }
    #     - { src: templates/named.conf.local.j2, dest: /etc/named/named.conf.local }
    #     - { src: templates/haproxy.cfg.j2, dest: /etc/haproxy/haproxy.cfg }
    #     - { src: templates/registry_pv.yaml.j2, dest: tmp }
    #     - { src: templates/install-config.yaml.j2, dest: install_dir/install-config.yaml }
    #   notify:
    #     - Restart DNS

    - import_tasks: tasks/dns-setup.yaml
      tags: ["dns"]

    # --------
    # - name: Pull secret checkpoint
    #   ansible.builtin.pause:
    #     prompt: "Go check the Ingress Load Balancer config file"

    # Run it more than once, and it'll add 80 ... and 80 ...
    - name: change httpd port
      lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: "^Listen.*80"
        line: "Listen 8080"
        backup: yes

    # And don't forget to restart it
    - name: enable services
      systemd:
        name: "{{ item }}"
        enabled: yes
        state: restarted
      with_items:
        - haproxy
        - httpd
        - tftp
        # - named
        # - dhcpd

- name: Setup OKD services
  hosts: service
  vars_files:
    - vars/main.yaml
    - vars/okd-version.yaml
    - vars/nodes.yaml
    - vars/network.yaml
  handlers:
    - import_tasks: handlers/main.yaml
  become: yes
  become_method: sudo
  gather_facts: no
  tasks:
    # Let's download the right files for our version of OKD
    - name: Upload OKD files to the service host
      copy:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        mode: "{{ item.mode }}"
      with_items:
        # - { src: files/client.tar.gz, dest: tmp/client.tar.gz, mode: 0644 }
        # - { src: files/openshift-install, dest: /usr/local/bin/openshift-install, mode: "0755" }
        # - { src: files/kernel, dest: tmp/kernel, mode: 0644 }
        # - { src: files/rootfs.img, dest: tmp/rootfs.img, mode: 0644 }
        # - { src: root/initramfs.img, dest: tmp/initramfs.img, mode: 0644 }
        - { src: "{{ jq_file }}", dest: tmp/jq, mode: "0644" }

    # This would be the perfect place to contact the registry server, and download the custom openshift-install
    # to allow full disconnected installation. That binary is generated by the mirror-registry, and contains
    # all the necessary info to allow connection to the local mirror. One generated for another version,
    # or another server, won't work (trust me, I tried ...). Of course, only if the install is disconnected
    # The command to generate it on the remote server :
    # oc adm release extract -a ${LOCAL_SECRET_JSON} --command=openshift-install "${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}-${ARCHITECTURE}" --insecure=true
    # export LOCAL_SECRET_JSON=~/.docker/config.json
    # export LOCAL_REGISTRY=the-reigstry-fqdn:8443
    # export LOCAL_REPOSITORY=openshift/release-images
    # export OCP_RELEASE=4.18.7
    # export ARCHITECTURE=x86_64

    # If not using mirror registry, the process is the same of OKD or OCP. But I'll need to do something for the creation of the mirror registry. It'll be for another time ...

    # - name: Download OCP files
    #   copy:
    #     src: "{{ item.src }}"
    #     dest: "{{ item.dest }}"
    #     mode: "{{ item.mode }}"
    #   with_items:
    #     - {
    #         src: "{{ versions[target][version].client }}",
    #         dest: tmp/client.tar.gz,
    #         mode: 0644,
    #       }
    #     - {
    #         src: "{{ versions[target][version].install }}",
    #         dest: /usr/local/bin/openshift-install,
    #         mode: "0755",
    #       }
    #     - {
    #         src: "{{ versions[target][version].kernel }}",
    #         dest: tmp/kernel,
    #         mode: 0644,
    #       }
    #     - {
    #         src: "{{ versions[target][version].rootfs }}",
    #         dest: tmp/rootfs.img,
    #         mode: 0644,
    #       }
    #     - {
    #         src: "{{ versions[target][version].initramfs }}",
    #         dest: tmp/initramfs.img,
    #         mode: 0644,
    #       }
    #     - { src: "{{ jq_file }}", dest: tmp/jq, mode: 0644 }
    #   when: target == "ocp"

    # - name: Extract OKD binaries (oc, kubectl) to /usr/local/bin
    #   unarchive:
    #     src: "{{ item.src }}"
    #     dest: /usr/local/bin
    #     remote_src: yes
    #   loop:
    #     - { src: "tmp/client.tar.gz" }

    - name: Copy image files to their proper location
      copy:
        remote_src: true
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
      loop:
        - { src: "tmp/kernel", dest: "/var/lib/tftpboot/fcos" }
        - { src: "tmp/initramfs.img", dest: "/var/lib/tftpboot/fcos" }
        - { src: "tmp/rootfs.img", dest: "/var/www/html/okd4" }
        - { src: "/usr/share/syslinux/", dest: "/var/lib/tftpboot" }

    - import_tasks: tasks/ssh-and-pull.yaml
      tags: ["pull-secret"]
    - import_tasks: tasks/prepare-manifests.yaml
      tags: ["manifests"]

    # # Let's create the proper OKD installation files (ignition, manifests, ...)
    # - name: Create the cluster manifests
    #   shell: |
    #     cp /home/ansible/install_dir/install-config.yaml /home/ansible/install-config.back
    #     /usr/local/bin/openshift-install create manifests --dir=/home/ansible/install_dir/

    # - name: Setting mastersSchedulable to false
    #   shell: |
    #     sed -i 's/mastersSchedulable: true/mastersSchedulable: False/' /home/ansible/install_dir/manifests/cluster-scheduler-02-config.yml
    #   when: masters_schedulable != true

    # - name: Create ignition files
    #   shell: /usr/local/bin/openshift-install create ignition-configs --dir=/home/ansible/install_dir/

    - import_tasks: tasks/prepare-webserver.yaml
      tags: ["web"]
    # # Make these files available on the Web server
    # - name: Copy all files to www directories
    #   shell: cp -R /home/ansible/install_dir/* /var/www/html/okd4/

    # - name: Change www ownership
    #   file:
    #     path: /var/www/html/okd4/
    #     recurse: yes
    #     state: directory
    #     owner: apache
    #     group: apache
    #     mode: 0755

    # - name: Change install_dir ownership
    #   file:
    #     path: /home/ansible/install_dir
    #     recurse: yes
    #     state: directory
    #     owner: ansible
    #     group: ansible
    #     mode: 0755

    - import_tasks: tasks/bootstrap.yaml
      tags: ["bootstrap"]

# At this point, all configs are created, and we should start to run the bootstrap command,
# manually fire up the bootstrap, then the control plane nodes, then run the final install command,
# boot the worker nodes, ...
# Let's automate all this ! This section will be divided between tasks on Proxmox, and tasks on the service machine

# To gain some time, we boot the bootstrap node before running the bootstrap command. That'll save us a few ... seconds ?

- name: Deploy bootstrap node
  hosts: proxmox
  vars_files:
    - vars/main.yaml
    - vars/nodes.yaml
    - vars/network.yaml
  become: yes
  become_method: sudo
  gather_facts: no
  tasks:
    - name: Start bootstrap
      shell: qm start "{{ bootstrap.vmid + vmbase}}"

# Bootstrap node is booting up, let's run the command that will create the tiny bootstrap cluster ...

- name: Start bootstrapping sequence
  hosts: service
  gather_facts: no
  tasks:
    - name: Starting bootstrap process
      shell: /usr/local/bin/openshift-install --dir=install_dir/ wait-for bootstrap-complete --log-level=debug > tmp/bootstrap.log 2>&1
      async: 3600
      poll: 0

    # Now we wait for the magiv message ending with "for bootstrapping to complete..." that indicates the bootstrap node is up and running
    - name: Wait for the message saying the control plane nodes should be started
      command:
        cmd: grep "for bootstrapping to complete..." tmp/bootstrap.log
      retries: 720
      delay: 5
      register: grp
      until: grp.stdout.find ("for bootstrapping to complete...") != -1

# Cool, at this stage, the bootstrap node is all good, and we can fire up the control plane nodes to join the fun !

- name: Deploy control plane nodes
  hosts: proxmox
  vars_files:
    - vars/main.yaml
    - vars/nodes.yaml
    - vars/network.yaml
  become: yes
  become_method: sudo
  gather_facts: no
  tasks:
    - name: Deploy OKD control plane nodes
      shell: qm start {{ item.vmid + vmbase}}
      loop: "{{ masters }}"

# The control plane nodes are now up and running, we just patiently wait for them to join the cluster
- name: Finish bootstrapping sequence
  hosts: service
  handlers:
    - import_tasks: handlers/main.yaml
  gather_facts: no
  become: yes
  become_method: sudo
  tasks:
    - name: Deploying control plane nodes
      command:
        cmd: grep "safe to remove the bootstrap" tmp/bootstrap.log
      retries: 720
      delay: 5
      register: grp
      until: grp.stdout.find ("safe to remove the bootstrap") != -1

    # Woohoo, the control plane nodes are up and running, and built our cluster. Let's kill the bootstrap node
    # But first, let's remove it from the HA proxy config !
    - name: Remove bootstrap from HA Proxy
      replace:
        path: /etc/haproxy/haproxy.cfg
        regexp: "^.*server.*bootstrap.*"
        replace: "# server bootstrap"
      notify:
        - Restart HA

# So let's take care of the bootstrap node. And fire up the worker nodes at the same time ...
- name: Deploy OKD worker nodes
  hosts: proxmox
  vars_files:
    - vars/nodes.yaml
    - vars/main.yaml
  become: yes
  become_method: sudo
  gather_facts: no
  tasks:
    - import_tasks: tasks/stop_bootstrap_start_workers.yaml

# In this section, we run the final piece of the installation, and we wait for the worker nodes to join the cluster.
- name: Wait for installation to be finished
  hosts: service
  gather_facts: no
  tasks:
    - import_tasks: tasks/workers.yaml
      tags: ["workers"]
